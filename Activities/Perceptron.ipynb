{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDnVPO1mrjVf3fttZurgM5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Esau-May/MachineLearningCourse/blob/main/Activities/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron**\n",
        "\n",
        "The Perceptron algorithm is a simple yet fundamental supervised learning technique for binary classification tasks. It aims to find a linear decision boundary that separates data points into two classes based on their features. The algorithm iteratively updates a set of weights and a bias term to adjust the decision boundary. During each iteration, the Perceptron evaluates data points and updates the weights and bias if a misclassification is encountered, effectively \"learning\" from its mistakes. The process continues until convergence or for a specified number of epochs."
      ],
      "metadata": {
        "id": "y0CSnTZxMRIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pseudocode**\n",
        "\n",
        "Import the necessary libraries, such as pandas, numpy, and matplotlib.\n",
        "\n",
        "Load a dataset from a CSV file named 'wdbc.data.'\n",
        "\n",
        "Convert the values in column 1 ('M' and 'B') into numerical values (1 and -1).\n",
        "\n",
        "Extract the features (columns 2 to 31) and labels (column 1) from the dataset.\n",
        "\n",
        "Calculate the number of samples for training and testing (80% for training, 20% for testing).\n",
        "\n",
        "Shuffle the data randomly to avoid biases.\n",
        "\n",
        "Split the data into training and testing sets.\n",
        "\n",
        "Define a function to train a perceptron, which adjusts weights and bias to classify the data.\n",
        "\n",
        "Train the perceptron with a learning rate of 0.1 over 100 epochs.\n",
        "\n",
        "Calculate the accuracy on the test set and a loss value.\n",
        "\n",
        "Print the perceptron's accuracy and the loss function value."
      ],
      "metadata": {
        "id": "5fhwp_fIJjGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargar el conjunto de datos desde el archivo .DATA\n",
        "data = pd.read_csv('wdbc.data', header=None)\n",
        "\n",
        "# Convertir valores con caracteristicas a numericas\n",
        "data[1] = data[1].map({'M': 1, 'B': -1})\n",
        "\n",
        "# Extraer las características y las etiquetas\n",
        "X = data.iloc[:, 2:32].to_numpy()\n",
        "y = data.iloc[:, 1].to_numpy()\n",
        "\n",
        "total_samples = X.shape[0]\n",
        "\n",
        "# Dividir el dataset en entrenamiento y test\n",
        "train_size = int(0.8 * total_samples)\n",
        "test_size = total_samples - train_size\n",
        "\n",
        "\n",
        "indices = np.arange(total_samples)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Dividir los datos\n",
        "X_train = X[indices[:train_size]]\n",
        "y_train = y[indices[:train_size]]\n",
        "X_test = X[indices[train_size:]]\n",
        "y_test = y[indices[train_size:]]\n",
        "\n",
        "# Definir la función de entrenamiento del perceptrón\n",
        "def train_perceptron(X, y, learning_rate, epochs):\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "    errors = []\n",
        "    for _ in range(epochs):\n",
        "        total_error = 0\n",
        "        for xi, target in zip(X, y):\n",
        "            update = learning_rate * (target - np.sign(np.dot(xi, w) + b))\n",
        "            w += update * xi\n",
        "            b += update\n",
        "            total_error += int(update != 0)\n",
        "        errors.append(total_error)\n",
        "    return w, b, errors\n",
        "\n",
        "# Entrenar el perceptrón\n",
        "learning_rate = 0.1\n",
        "epochs = 100\n",
        "weights, bias, errors = train_perceptron(X_train, y_train, learning_rate, epochs)\n",
        "\n",
        "\n",
        "# Calcular la precisión en el conjunto de prueba\n",
        "def predict(X, weights, bias):\n",
        "    z = np.dot(X, weights) + bias\n",
        "    return np.sign(z)\n",
        "\n",
        "def loss(X, y, weights, bias):\n",
        "\n",
        "    y_pred = np.sign(np.dot(X, weights) + bias)\n",
        "\n",
        "\n",
        "    hinge_loss = np.maximum(0, 1 - y * y_pred)\n",
        "\n",
        "\n",
        "    average_loss = np.mean(hinge_loss)\n",
        "\n",
        "    return average_loss\n",
        "\n",
        "\n",
        "y_pred = predict(X_test, weights, bias)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "loss_value = loss(X_test, y_test, weights, bias)\n",
        "\n",
        "print(f'Precisión del Perceptrón: {accuracy * 100:.2f}%')\n",
        "print(f'Valor de la Loss function: {loss_value:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0uelQ61CUEe",
        "outputId": "4507bef7-57db-4050-edc4-5f136ddc9aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión del Perceptrón: 88.60%\n",
            "Valor de la Loss function: 0.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss function**\n",
        "\n",
        "The loss function in this code is the Hinge Loss, which is used to measure the error between the predicted values and the true labels in the context of binary classification. The Hinge Loss is defined as the maximum of zero and the difference between 1 and the product of the true label and the predicted value. This loss function penalizes the model when its predictions are on the wrong side of the decision boundary. If the predicted value and the true label have the same sign and are on the correct side of the boundary, the loss is zero, indicating that the prediction is accurate."
      ],
      "metadata": {
        "id": "kqFZ0nJaI2Ml"
      }
    }
  ]
}