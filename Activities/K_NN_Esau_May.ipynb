{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEfjdqpJPpR+FD/t/Oonmh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Esau-May/MachineLearningCourse/blob/main/Activities/K_NN_Esau_May.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**k-NN**\n",
        "\n",
        "The k-NN (k-Nearest Neighbors) algorithm is a classification and regression method that is based on the proximity between data. In the prediction process, it searches for the k closest examples in the training set and makes a decision based on the labels of those neighbors."
      ],
      "metadata": {
        "id": "_jA9rZNSBucP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pseudocode**\n",
        "\n",
        "Load the Abalone dataset from a CSV file named 'abalone.data' and assign it to the variable 'data' using the pandas library.\n",
        "\n",
        "Define a list called 'column_names' containing the names of the columns in the dataset.\n",
        "\n",
        "Modify the 'Sex' column in the dataset to assign numerical values to the categories 'M', 'F' and 'I'.\n",
        "\n",
        "Separate the features (X) and labels (y) of the dataset.\n",
        "\n",
        "Define a function called 'euclidean_distance' to calculate the Euclidean distance between two points.\n",
        "\n",
        "Define a function called 'knn_predict' that predicts the classification using the k-NN algorithm.\n",
        "\n",
        "Split the dataset into a training set (X_train, y_train) and a test set (X_test, y_test) using partitioning.\n",
        "\n",
        "Define a value 'k_value' for the number of k-nearest neighbors in the k-NN algorithm.\n",
        "\n",
        "Initialize a counter 'correct_predictions' to keep track of correct predictions.\n",
        "\n",
        "Calculate the total number of predictions in the test set.\n",
        "\n",
        "For each point in the test set:\n",
        "\n",
        "- Use the 'knn_predict' function to predict the class.\n",
        "\n",
        "- Compare the prediction with the actual label in the test set.\n",
        "\n",
        "- If the prediction is correct, increment the counter 'correct_predictions'.\n",
        "\n",
        "Calculate the accuracy of the k-NN model by dividing 'correct_predictions' by the total number of predictions in the test set.\n",
        "\n",
        "Print the class prediction for the last point in the test set.\n",
        "\n",
        "Print the accuracy of the k-NN model to two decimal places"
      ],
      "metadata": {
        "id": "-dqqFuo7_M9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "column_names = ['Sex', 'Length', 'Diameter', 'Height', 'WholeWeight', 'ShuckedWeight', 'VisceraWeight', 'ShellWeight', 'Rings']\n",
        "data = pd.read_csv('abalone.data', names=column_names)\n",
        "\n",
        "\n",
        "data['Sex'] = data['Sex'].map({'M': 0, 'F': 1, 'I': 2})\n",
        "\n",
        "\n",
        "X = data.drop('Rings', axis=1).values\n",
        "y = data['Rings'].values\n",
        "\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
        "\n",
        "\n",
        "def knn_predict(X_train, y_train, x_test, k):\n",
        "    distances = [euclidean_distance(x, x_test) for x in X_train]\n",
        "    k_indices = np.argsort(distances)[:k]\n",
        "    k_nearest_labels = [y_train[i] for i in k_indices]\n",
        "    most_common = np.bincount(k_nearest_labels).argmax()\n",
        "    return most_common\n",
        "\n",
        "\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(X) * split_ratio)\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "\n",
        "k_value = 10\n",
        "correct_predictions = 0\n",
        "total_predictions = len(X_test)\n",
        "\n",
        "for i in range(total_predictions):\n",
        "    predicted_class = knn_predict(X_train, y_train, X_test[i], k_value)\n",
        "    if predicted_class == y_test[i]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f'Class prediction using k-NN: {predicted_class}')\n",
        "print(f'k-NN model accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP0_qJLuMH8C",
        "outputId": "b7a81cd8-2c2e-45b4-da9e-121265727f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class prediction using k-NN: 11\n",
            "k-NN model accuracy: 0.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function and Optimization Function**\n",
        "\n",
        "Loss and optimization functions are fundamental in machine learning, but are not used in a k-NN-based classification code. K-NN is a \"lazy learning\" algorithm that does not involve training with parameter optimization. Instead, k-NN stores training data and makes predictions based on nearest-neighbor proximity. Loss and optimization functions are features of algorithms that adjust parameters through an optimization process to minimize error, whereas k-NN is based on similarity and does not perform this explicit optimization."
      ],
      "metadata": {
        "id": "MVT7dexbC1tT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!jupyter nbconvert --to html DecisionTree_Regression_EsauMay.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qagZWQ7eFCD6",
        "outputId": "4bac0489-2d42-4ae2-e398-7c21a0a6b539"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook DecisionTree_Regression_EsauMay.ipynb to html\n",
            "[NbConvertApp] Writing 817565 bytes to DecisionTree_Regression_EsauMay.html\n"
          ]
        }
      ]
    }
  ]
}